{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94by_5kdSckY"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GaeK6qvpYzI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW51RaqSX9oi"
      },
      "source": [
        "**Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exLhBZXxYDJO"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/Alexa.csv\"\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/dga.csv\"\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/extra.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5woD1rZY08B"
      },
      "source": [
        "**Loading the initial Shape of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T9mq6twYaNi",
        "outputId": "902eddc3-5714-4fff-a3d5-286287414fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of Alexa dataset:\n",
            "    0              1\n",
            "0  1     google.com\n",
            "1  2    youtube.com\n",
            "2  3   facebook.com\n",
            "3  4      baidu.com\n",
            "4  5  wikipedia.org\n",
            "\n",
            "First few rows of DGA dataset:\n",
            "              rank  domain\n",
            "0          domain     NaN\n",
            "1  mzvbfkkoij.com     NaN\n",
            "2  dxczoqvzpc.com     NaN\n",
            "3  gezufojmci.com     NaN\n",
            "4  vgtsavhzfg.com     NaN\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Alexa dataset without assuming headers\n",
        "alexa_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Alexa.csv', header=None)\n",
        "\n",
        "# Load DGA dataset with correct headers\n",
        "dga_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dga.csv', names=['rank', 'domain'])\n",
        "\n",
        "# Display first few rows\n",
        "print(\"First few rows of Alexa dataset:\\n\", alexa_df.head())\n",
        "print(\"\\nFirst few rows of DGA dataset:\\n\", dga_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND7sND9IB8nT",
        "outputId": "7196272f-e86d-46d1-c8f7-2a92203bf65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned DGA dataset shape: (499002, 1)\n",
            "First few rows of cleaned DGA dataset:\n",
            "            domain\n",
            "0          domain\n",
            "1  mzvbfkkoij.com\n",
            "2  dxczoqvzpc.com\n",
            "3  gezufojmci.com\n",
            "4  vgtsavhzfg.com\n"
          ]
        }
      ],
      "source": [
        "# Reload DGA dataset if needed\n",
        "dga_raw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dga.csv', names=['rank', 'domain'], header=None)\n",
        "\n",
        "# Move valid domain names from 'rank' column to 'domain' column\n",
        "dga_raw_df['domain'] = dga_raw_df['rank']\n",
        "\n",
        "# Now drop the 'rank' column as it's no longer needed\n",
        "dga_cleaned_df = dga_raw_df.drop(columns=['rank'])\n",
        "\n",
        "# Remove rows where the 'domain' column is NaN\n",
        "dga_cleaned_df = dga_cleaned_df[dga_cleaned_df['domain'].notna()]\n",
        "\n",
        "# Convert all domain names to lowercase for consistency\n",
        "dga_cleaned_df['domain'] = dga_cleaned_df['domain'].str.lower()\n",
        "\n",
        "# Verify the cleaned dataset\n",
        "print(\"Cleaned DGA dataset shape:\", dga_cleaned_df.shape)\n",
        "print(\"First few rows of cleaned DGA dataset:\\n\", dga_cleaned_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbUTkzRaCp6L",
        "outputId": "4d24c7b4-dc69-444a-fd92-3fc8dd6b54a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned DGA dataset shape: (499001, 1)\n",
            "Cleaned Alexa dataset shape: (1000000, 1)\n",
            "\n",
            "First few rows of Alexa dataset:\n",
            "           domain\n",
            "0     google.com\n",
            "1    youtube.com\n",
            "2   facebook.com\n",
            "3      baidu.com\n",
            "4  wikipedia.org\n",
            "First few rows of cleaned DGA dataset:\n",
            "            domain\n",
            "1  mzvbfkkoij.com\n",
            "2  dxczoqvzpc.com\n",
            "3  gezufojmci.com\n",
            "4  vgtsavhzfg.com\n",
            "5  fvbvfmwlcn.com\n"
          ]
        }
      ],
      "source": [
        "# Keep only the domain column\n",
        "alexa_df = alexa_df[[1]]  # Select the correct column\n",
        "alexa_df.columns = [\"domain\"]  # Rename column\n",
        "# Remove the first row (which contains 'domain' as a value)\n",
        "dga_cleaned_df = dga_cleaned_df[dga_cleaned_df['domain'] != \"domain\"]\n",
        "\n",
        "# Verify the cleaned DGA dataset\n",
        "print(\"Cleaned DGA dataset shape:\", dga_cleaned_df.shape)\n",
        "print(\"Cleaned Alexa dataset shape:\", alexa_df.shape)\n",
        "# Print first few rows of Alexa dataset\n",
        "print(\"\\nFirst few rows of Alexa dataset:\\n\", alexa_df.head())\n",
        "# Print first few rows of Cleaned DGA dataset\n",
        "print(\"First few rows of cleaned DGA dataset:\\n\", dga_cleaned_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvZ7XOkLJF5G",
        "outputId": "5bc02f89-e5be-4054-fc5c-f8859c06db8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned DGA dataset shape: (499001, 1)\n",
            "Cleaned Alexa dataset shape: (1000000, 1)\n",
            "\n",
            "First few rows of Alexa dataset:\n",
            "           domain\n",
            "0     google.com\n",
            "1    youtube.com\n",
            "2   facebook.com\n",
            "3      baidu.com\n",
            "4  wikipedia.org\n",
            "First few rows of cleaned DGA dataset:\n",
            "            domain\n",
            "1  mzvbfkkoij.com\n",
            "2  dxczoqvzpc.com\n",
            "3  gezufojmci.com\n",
            "4  vgtsavhzfg.com\n",
            "5  fvbvfmwlcn.com\n"
          ]
        }
      ],
      "source": [
        "# Assuming dga_cleaned_df and alexa_df are already loaded\n",
        "# Remove the first row (which contains 'domain' as a value)\n",
        "dga_cleaned_df = dga_cleaned_df[dga_cleaned_df['domain'] != \"domain\"]\n",
        "\n",
        "# Verify the cleaned DGA dataset\n",
        "print(\"Cleaned DGA dataset shape:\", dga_cleaned_df.shape)\n",
        "print(\"Cleaned Alexa dataset shape:\", alexa_df.shape)\n",
        "\n",
        "# Print first few rows of Alexa dataset\n",
        "print(\"\\nFirst few rows of Alexa dataset:\\n\", alexa_df.head())\n",
        "\n",
        "# Print first few rows of Cleaned DGA dataset\n",
        "print(\"First few rows of cleaned DGA dataset:\\n\", dga_cleaned_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP8Cv2tZP5oo"
      },
      "source": [
        "**Added Extra data from 360**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "478sLf7MP-Bo",
        "outputId": "0a6cc16d-0773-46d3-f279-30e0df064af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Additional DGA dataset shape: (10000, 1)\n",
            "First few rows of additional DGA dataset:\n",
            "                              Domain\n",
            "0               suggestmoredue.link\n",
            "1    admin.leaveacceptablerock.link\n",
            "2              meanhardpositive.net\n",
            "3  server.killcontentexternal.click\n",
            "4            hearconsciousowner.net\n"
          ]
        }
      ],
      "source": [
        "# Load the additional DGA data\n",
        "extra_dga_path = \"/content/drive/MyDrive/Colab Notebooks/extra.csv\"\n",
        "extra_dga_df = pd.read_csv(extra_dga_path)\n",
        "\n",
        "# Verify the additional DGA data\n",
        "print(\"Additional DGA dataset shape:\", extra_dga_df.shape)\n",
        "print(\"First few rows of additional DGA dataset:\\n\", extra_dga_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZUImoRvQFZ7"
      },
      "source": [
        "**Combine the Additional DGA Data with the Existing DGA Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5pUtLsQRyUR",
        "outputId": "a1bb532e-eeec-4982-d572-183d3965e5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined DGA dataset shape: (509001, 1)\n"
          ]
        }
      ],
      "source": [
        "# Ensure the column names match\n",
        "if 'domain' not in extra_dga_df.columns:\n",
        "    extra_dga_df.rename(columns={extra_dga_df.columns[0]: 'domain'}, inplace=True)\n",
        "\n",
        "# Combine the additional DGA data with the existing DGA data\n",
        "dga_combined_df = pd.concat([dga_cleaned_df, extra_dga_df], ignore_index=True)\n",
        "\n",
        "# Verify the combined DGA dataset\n",
        "print(\"Combined DGA dataset shape:\", dga_combined_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPUaQT7bTLFa",
        "outputId": "1aad86f9-f295-4994-a113-fbf1788c4270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labeled DGA dataset shape: (509001, 2)\n",
            "First few rows of labeled DGA dataset:\n",
            "            domain  label\n",
            "0  mzvbfkkoij.com      1\n",
            "1  dxczoqvzpc.com      1\n",
            "2  gezufojmci.com      1\n",
            "3  vgtsavhzfg.com      1\n",
            "4  fvbvfmwlcn.com      1\n"
          ]
        }
      ],
      "source": [
        "# Add labels to the combined DGA dataset\n",
        "dga_combined_df['label'] = 1  # DGA domains are labeled as 1\n",
        "\n",
        "# Verify the labeled DGA dataset\n",
        "print(\"Labeled DGA dataset shape:\", dga_combined_df.shape)\n",
        "print(\"First few rows of labeled DGA dataset:\\n\", dga_combined_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDHXJY9PWiKF",
        "outputId": "e9d81548-f9c1-4ba4-b608-844b5b091399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned Alexa dataset shape: (1000000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"Cleaned Alexa dataset shape:\", alexa_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz1Ba8sFJYDm",
        "outputId": "d14b548a-5af3-4f5a-9b8e-c818e00b2b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataset shape: (1509001, 2)\n",
            "First few rows of combined dataset:\n",
            "            domain  label\n",
            "0  mzvbfkkoij.com      1\n",
            "1  dxczoqvzpc.com      1\n",
            "2  gezufojmci.com      1\n",
            "3  vgtsavhzfg.com      1\n",
            "4  fvbvfmwlcn.com      1\n"
          ]
        }
      ],
      "source": [
        "# Add labels to the datasets\n",
        "dga_combined_df['label'] = 1  # DGA domains are labeled as 1\n",
        "alexa_df['label'] = 0        # Alexa domains are labeled as 0\n",
        "\n",
        "# Combine the datasets\n",
        "combined_df = pd.concat([dga_combined_df, alexa_df ], ignore_index=True)\n",
        "\n",
        "# Verify the combined dataset\n",
        "print(\"Combined dataset shape:\", combined_df.shape)\n",
        "print(\"First few rows of combined dataset:\\n\", combined_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXGnVwUWTwj"
      },
      "source": [
        "**Balancing the Datasets to 500k each**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-3_vCv9WYwk",
        "outputId": "27442f4e-4521-4d20-f300-992e13271540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced dataset shape: (1000000, 2)\n",
            "Class distribution:\n",
            " label\n",
            "0    500000\n",
            "1    500000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Separate the datasets\n",
        "dga_df = combined_df[combined_df['label'] == 1]\n",
        "alexa_df = combined_df[combined_df['label'] == 0]\n",
        "\n",
        "# Downsample DGA data to 500,000 samples (if it has more)\n",
        "if len(dga_df) > 500000:\n",
        "    dga_df = resample(dga_df, replace=False, n_samples=500000, random_state=42)\n",
        "\n",
        "# Downsample Alexa data to 500,000 samples\n",
        "alexa_df = resample(alexa_df, replace=False, n_samples=500000, random_state=42)\n",
        "\n",
        "# Combine the balanced datasets\n",
        "balanced_df = pd.concat([dga_df, alexa_df], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Verify the balanced dataset\n",
        "print(\"Balanced dataset shape:\", balanced_df.shape)\n",
        "print(\"Class distribution:\\n\", balanced_df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lOHzMelcged"
      },
      "source": [
        "**Save the Balanced Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ActvsnclO8",
        "outputId": "d8b5e07c-de23-4970-9def-8634c46325f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset saved to /content/drive/MyDrive/Colab Notebooks/balanced_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "# Save the balanced dataset to Google Drive\n",
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/balanced_dataset.csv\"\n",
        "balanced_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Dataset saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQCTiM-Whg1"
      },
      "source": [
        "**Split the Data into Training and Testing Sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0KlKHgAWjAT"
      },
      "source": [
        "Split the balanced dataset into training (80%) and testing (20%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzm2uye-WnSM",
        "outputId": "cc9bd59b-6536-46ca-a07a-22b7685bb4c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (800000,)\n",
            "Testing data shape: (200000,)\n"
          ]
        }
      ],
      "source": [
        "# Split the data into features (X) and labels (y)\n",
        "X = balanced_df['domain']\n",
        "y = balanced_df['label']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the splits\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocz_RtR-XwaK"
      },
      "source": [
        "**Feature Extraction for Each Model Component**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVCtdCO3XyG2"
      },
      "source": [
        "**1. FastText Embaddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFweesotBU2H",
        "outputId": "aeae2a1c-b741-4652-cdf8-46b734ca5fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313473 sha256=013d270b20429eddc4a47882b21d2e2e9491df8e0436d6f1a68af5beeb4fb304\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "Training FastText embeddings shape: (800000, 100)\n",
            "Testing FastText embeddings shape: (200000, 100)\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# Save the training data to a temporary file\n",
        "with open(\"train_data.txt\", \"w\") as f:\n",
        "    for domain in X_train.tolist():\n",
        "        f.write(domain + \"\\n\")\n",
        "\n",
        "# Train FastText model on the temporary file\n",
        "fasttext_model = fasttext.train_unsupervised(\"train_data.txt\", model='skipgram', dim=100)\n",
        "\n",
        "# Function to generate FastText embeddings\n",
        "def get_fasttext_embeddings(texts, model):\n",
        "    return np.array([model.get_word_vector(text) for text in texts])\n",
        "\n",
        "# Generate FastText embeddings for training and testing data\n",
        "X_train_fasttext = get_fasttext_embeddings(X_train.tolist(), fasttext_model)\n",
        "X_test_fasttext = get_fasttext_embeddings(X_test.tolist(), fasttext_model)\n",
        "\n",
        "# Verify the embeddings\n",
        "print(\"Training FastText embeddings shape:\", X_train_fasttext.shape)\n",
        "print(\"Testing FastText embeddings shape:\", X_test_fasttext.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kuc9O0-hpMQ"
      },
      "source": [
        " **2. Graph CNN\n",
        "Convert domain names into character-level graphs.t**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsP1NGu7hvvY"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert domain names to graphs\n",
        "def domain_to_graph(domain):\n",
        "    G = nx.Graph()\n",
        "    for i in range(len(domain) - 1):\n",
        "        G.add_edge(domain[i], domain[i + 1])\n",
        "    return G\n",
        "\n",
        "# Convert training and testing data to graphs\n",
        "X_train_graphs = [domain_to_graph(domain) for domain in X_train]\n",
        "X_test_graphs = [domain_to_graph(domain) for domain in X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkfYS5dwiLlp"
      },
      "source": [
        " **3. LSTM (LLN)\n",
        "Use character-level sequences for LSTM.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q7NdXadiUvs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize characters in domain names\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert domain names to sequences of character indices\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_len = 50\n",
        "X_train_seq = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_seq = pad_sequences(X_test_seq, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORaZO_a5C1o4"
      },
      "source": [
        "**4. SVM (Traditional Features)\n",
        "Extract traditional features like domain length, entropy, and n-grams. **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7Ky9NDqC8h9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert domain names into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liqI4558DaHb"
      },
      "source": [
        "**Step 3: Build the Hybrid Model\n",
        "Combine the outputs of FastText, Graph CNN, LSTM, and SVM using a meta-classifier.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EBrENGfDgaf",
        "outputId": "5cbe35d9-e87a-4d17-8fa1-a9f0a21d0760"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 45ms/step - accuracy: 0.8475 - loss: 0.3278 - val_accuracy: 0.9302 - val_loss: 0.1704\n",
            "Epoch 2/5\n",
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 46ms/step - accuracy: 0.9340 - loss: 0.1624 - val_accuracy: 0.9441 - val_loss: 0.1415\n",
            "Epoch 3/5\n",
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 40ms/step - accuracy: 0.9482 - loss: 0.1322 - val_accuracy: 0.9527 - val_loss: 0.1244\n",
            "Epoch 4/5\n",
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 41ms/step - accuracy: 0.9565 - loss: 0.1145 - val_accuracy: 0.9556 - val_loss: 0.1153\n",
            "Epoch 5/5\n",
            "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 41ms/step - accuracy: 0.9608 - loss: 0.1033 - val_accuracy: 0.9596 - val_loss: 0.1050\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Define individual models\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Define LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_len),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the LSTM model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "lstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Combine models using a stacking classifier\n",
        "meta_classifier = LogisticRegression()\n",
        "hybrid_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('svm', svm_model),\n",
        "        ('lstm', lstm_model)\n",
        "    ],\n",
        "    final_estimator=meta_classifier\n",
        ")\n",
        "\n",
        "# Train the hybrid model\n",
        "hybrid_model.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYXMU_CL6rIx"
      },
      "source": [
        "**1: Predict on Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reHHMiyb6tmD"
      },
      "outputs": [],
      "source": [
        "# Predict on test data\n",
        "y_pred = hybrid_model.predict(X_test_tfidf)\n",
        "\n",
        "# Predict probabilities for ROC curve\n",
        "y_pred_proba = hybrid_model.predict_proba(X_test_tfidf)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcLaTppd60py"
      },
      "source": [
        "Calculate Evaluation Metrics\n",
        "Compute accuracy, precision,** recall, F1-score, and AUC-ROC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meBUz8pT6-4_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"AUC-ROC:\", roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcGtqN9s7kKR"
      },
      "source": [
        "**Generate a Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOy2K5rf7nDa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred, target_names=[\"Legitimate\", \"DGA\"])\n",
        "print(\"Classification Report:\\n\", class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msV8retj7uT_"
      },
      "source": [
        "**4: Plot the ROC Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOuwes867wVM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDj1cNMG75jO"
      },
      "source": [
        "** 5: Plot the Precision-Recall Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC1eHcI5778V"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEgFjvQ38FpI"
      },
      "source": [
        "** the Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcSpUWVx8H2i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=[\"Legitimate\", \"DGA\"],\n",
        "            yticklabels=[\"Legitimate\", \"DGA\"])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xctPXG88NAe"
      },
      "source": [
        "**Features Importances**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4KpgsmG8QSi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get feature importances from the SVM model\n",
        "feature_importances = svm_model.coef_[0]\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(feature_importances)), feature_importances)\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importances from SVM')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX8_LCYA8i6f"
      },
      "source": [
        "**Evaluation Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsG5NdyF8k9w"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save evaluation metrics to a JSON file\n",
        "evaluation_metrics = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"F1-Score\": f1,\n",
        "    \"AUC-ROC\": roc_auc\n",
        "}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/evaluation_metrics.json\", \"w\") as f:\n",
        "    json.dump(evaluation_metrics, f, indent=4)\n",
        "\n",
        "# Save classification report to a text file\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/classification_report.txt\", \"w\") as f:\n",
        "    f.write(class_report)\n",
        "\n",
        "# Save ROC curve plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.savefig(\"/content/drive/MyDrive/Colab Notebooks/roc_curve.png\")\n",
        "plt.close()\n",
        "\n",
        "# Save confusion matrix plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=[\"Legitimate\", \"DGA\"],\n",
        "            yticklabels=[\"Legitimate\", \"DGA\"])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(\"/content/drive/MyDrive/Colab Notebooks/confusion_matrix.png\")\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}